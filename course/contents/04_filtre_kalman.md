---
title: Filtre de Kalman
subtitle: Filtrage et mod√©lisation stochastique
---

**Filtre de Kalman** (1960), Rudolf Kalman (math√©maticien hongrois) : utilis√© pour la premi√®re fois pour l'estimation de trajectoire des programmes Apollo.

$\rightarrow$ Comment concilier au mieux l'information disponible (capteurs), et les √©quations de la dynamique pour contr√¥ler un syst√®me ?

# Contr√¥le d'un syst√®me dynamique

**Syst√®me dynamique discret**

$$x_{n+1} = A x_n + B u_n + \phi_n$$
$$y_{n+1} = C x_{n+1} + \psi_{n+1}$$

o√π, √† chaque instant $n$:
- $x_n$ est l'√©tat du syst√®me. _Exemple : Temp√©rature d'un moteur d'avion_. 
- $u_n$ est la commande du syst√®me. _Exemple : D√©bit de carburant dans le moteur_.
- $y_n$ est la mesure de l'√©tat du syst√®me. _Exemple : Mesure renvoy√©e par le thermom√®tre_.


:::{note} Notations en m√©t√©orologie
En m√©t√©o, on appellerait $C$ un **op√©rateur d'observation**. C'est le lien entre **l'espace des mesures** (ex : la tension au bornes thermom√®tre), et **l'espace d'√©tat** (ex : la temp√©rature effective de l'air). 
:::

## Bruits associ√©s au syst√®me

**Syst√®me dynamique discret**

$$x_{n+1} = A x_n + B u_n + \phi_n$$
$$y_{n+1} = C x_{n+1} + \psi_{n+1}$$

**Repr√©sentation des bruits** :
1. **Bruit d'√©tat** $\phi_n$: repr√©sente notre m√©connaissance de la physique du syst√®me. 
    - _Exemple : Variations de la richesse du m√©lange_.
2. **Bruit de mesure** $\psi_n$: repr√©sente un bruit de mesure. 
    - _Exemple: bruit √©lectronique de la sonde de temp√©rature_.


**Hypoth√®se** : Les bruits $\phi$ et $\psi$ sont suppos√©s blancs, gaussiens, centr√©s, stationnaires
et ind√©pendants l'un de l'autre. Ces bruit sont chacun associ√© √† une matrice de covariance $\Phi$, et $\Psi$. 


# Construction d'un estimateur

On cherche √† construire un estimateur qui d√©pende de l'√©tat estim√© √† l'instant pr√©c√©dent, de la mesure renoy√©e 
par le capteur et de la comande impos√©e.

Nous construisons alors un estimateur de la forme :

$$ \hat{x}_{n+1} = A_f \hat{x}_n + B_f u_n + K_{n+1} y_{n+1}$$

$\rightarrow$ Comment construire $A_f$, $B_f$, $K_{n+1}$ ?

:::{note} Construction de l'estimateur
On cherche √† construire un estimateur de la forme g√©n√©rique $\hat{x}_{n+1} = f(\hat{x}_n, y_{n+1}, u_n)$.

Nous nous concentrons sur des syst√®mes lin√©aires, et verrons plus tard comment l'√©tendre √† des syst√®mes non-lin√©aires.
:::



## Estimateur (assymptotiquement) sans biais

**Erreur d'estimation** :  On cherche √† minimiser l'erreur d'estimation, d√©finie ci-dessous,

$$e = \hat{x} - x$$

Pour un estimateur parfait, on aurait $\forall n \in \mathbb{N}, e_n = 0$. 

Sans acc√®s aux vraies valeurs de 
$x$ et face √† un processus stochastique, annuler l'erreur est g√©n√©ralement impossible.
Nous d√©crivons alors (non sans mal) l'erreur $e_{n}$ par r√©currence:

$$e_{n+1} = \begin{array}{rcl}
(I - K_{n+1} C) A e_n
+ (A_f + K_{n+1} CA -A) \hat{x}_n + (B_f +K_{n+1}CB - B) u_{n} \\
+ (K_{n+1} C - I) \phi_n + K_{n+1} \psi_{n+1}
\end{array}$$

**Esp√©rance de l'erreur** Sachant que les bruits blancs gaussiens sont d'esp√©rance nulle : 


$\forall n \in \mathbb{N}, \, \mathbb{E}[\phi_n] = 0, \mathbb{E}[\psi_n] = 0$, on d√©crit l'√©volution, l'esp√©rance de l'erreur.

$$\mathbb{E}[e_{n+1}] = (I - K_{n+1}C) A\, \mathbb{E}[e_n] + (A_f + K_{n+1}CA - A)\, \hat{x}_n + (B_f + K_{n+1} CB - B) u_n$$

**Estimateur assymptotiquement sans biais** 

On cherche √† construire un estimateur dont l'esp√©rance de l'erreur tend vers 0.  


**D√©finition** 
$$\lim_{n \rightarrow \infty} \mathbb{E}[e_n] = 0$$


**Esp√©rance de l'erreur** Sachant que les bruits blancs gaussiens sont d'esp√©rance nulle : 


$\forall n \in \mathbb{N}, \, \mathbb{E}[\phi_n] = 0, \mathbb{E}[\psi_n]$, on d√©crit l'√©volution, l'esp√©rance de l'erreur.

$$\mathbb{E}[e_{n+1}] = (I - K_{n+1}C) A\, \mathbb{E}[e_n] + (A_f + K_{n+1}CA - A)\, \hat{x}_n + (B_f + K_{n+1} CB - B) u_n$$

**Estimateur assymptotiquement sans biais**  $\rightarrow\, \lim\limits_{n \rightarrow \infty} \mathbb{E}[e_n] = 0$

**Conditions suffisantes**
1. $B_f + K_{n+1}\, CB - B = 0$ (_on annule le terme li√© √† la comande $u_n$_)
2. $A_f + K_{n+1}\, CA - A = 0$ (_on annule le terme d√ª √† l'estimation $\hat{x}_n$_)
3. $(I - K_{n+1}C)$ est stable (_module inf√©rieur √† 1 pour que l'erreur puisse d√©cro√Ætre )_ 




## Construction des Matrices du filtre
Filtre de Kalman

Les conditions de stabilit√© fixent les matrices $A_f$ et $B_f$.

$$A_f = (I - K_{n+1}C)A$$
$$B_f = (I - K_{n+1}C)B$$

Il reste √† r√©gler $K_{n+1}$ pour que $(I - K_{n+1}C)$ soit stable.

**D√©finition** : $K$ est appel√© le **gain du filtre**.

**Forme de l'estimateur** : Nous √©tablissons (et rencontrons souvent) la forme suivante du filtre.

$$\hat{x}_{n+1} = A \hat{x}_n + B u_n + K_{n+1} [y_{n+1} - C(A \hat{x}_n + B u_n)]$$


:::{note} Gain du filtre : un compromis 
**Remarque** $K$ peut √™tre vu comme un compromis √† r√©gler entre la fid√©lit√© au mod√®le num√©rique ($A \hat{x}_n + B u_n$) et la
fid√©lit√© aux valeurs de mesure ($y_{n+1}$). 
:::

## Filtre Pr√©dicteur - Correcteur

Le filtre de Kalman est un filtre **pr√©dicteur-correcteur**, l'estimation $\hat{x}$ de $x$ se contruit en 2 temps :

**Mise √† jour de l'√©tat** :

1. **Pr√©diction** : Estimation √† priori de l'√©tat $\hat{x}^-$, 
comme si on n'avait que les √©quations du syst√®me
√† disposition.

2. **Correction** : Construction de l'estimation √† posteriori avec l'information apport√©e par les mesures.


**Mise √† jour des matrices de covariance** (li√©es au bruit ajout√© √† chaque pas) :

1. **Pr√©diction** : Estimation de la matrice de covariance $P_k^-$,
par rapport au **bruit d'√©tat** $Q$ (ou $\Phi$ comme not√© pr√©c√©demment).   

2. **Correction** : Construction de la matrice de covariance $P_k$ par rapport au **bruit de mesure** $R$ (ou $\Psi$ comme not√© pr√©c√©demment).


### Sch√©ma de la boucle Pr√©dicteur-Correcteur

```mermaid
flowchart TB
    subgraph INIT["üîß INITIALISATION"]
        direction TB
        I1["√âtat initial<br/>xÃÇ‚ÇÄ"]
        I2["Covariance initiale<br/>P‚ÇÄ"]
    end

    subgraph PREDICT["üìä √âTAPE DE PR√âDICTION (Time Update)"]
        direction TB
        P1["<b>√âtat a priori</b><br/>xÃÇ‚Çô‚Çä‚ÇÅ‚Åª = A xÃÇ‚Çô + B u‚Çô"]
        P2["<b>Covariance a priori</b><br/>P‚Çô‚Çä‚ÇÅ‚Åª = A P‚Çô A·µÄ + Œ¶"]
        P1 --> P2
    end

    subgraph CORRECT["üìê √âTAPE DE CORRECTION (Measurement Update)"]
        direction TB
        C1["<b>Gain de Kalman</b><br/>K‚Çô‚Çä‚ÇÅ = P‚Çô‚Çä‚ÇÅ‚Åª C·µÄ (C P‚Çô‚Çä‚ÇÅ‚Åª C·µÄ + Œ®)‚Åª¬π"]
        C2["<b>Innovation</b><br/>·ªπ‚Çô‚Çä‚ÇÅ = y‚Çô‚Çä‚ÇÅ - C xÃÇ‚Çô‚Çä‚ÇÅ‚Åª"]
        C3["<b>√âtat a posteriori</b><br/>xÃÇ‚Çô‚Çä‚ÇÅ = xÃÇ‚Çô‚Çä‚ÇÅ‚Åª + K‚Çô‚Çä‚ÇÅ ·ªπ‚Çô‚Çä‚ÇÅ"]
        C4["<b>Covariance a posteriori</b><br/>P‚Çô‚Çä‚ÇÅ = (I - K‚Çô‚Çä‚ÇÅ C) P‚Çô‚Çä‚ÇÅ‚Åª"]
        C1 --> C2 --> C3 --> C4
    end

    subgraph MEASURE["üì° MESURE"]
        M1["Nouvelle observation<br/>y‚Çô‚Çä‚ÇÅ"]
    end

    INIT --> PREDICT
    PREDICT --> CORRECT
    MEASURE --> CORRECT
    CORRECT -->|"n ‚Üê n+1"| PREDICT

    style INIT fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style PREDICT fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style CORRECT fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style MEASURE fill:#fce4ec,stroke:#c2185b,stroke-width:2px
```

### Flux de donn√©es d√©taill√©

```mermaid
flowchart LR
    subgraph Entr√©es
        X0["xÃÇ‚ÇÄ, P‚ÇÄ"]
        U["Commande u‚Çô"]
        Y["Mesure y‚Çô‚Çä‚ÇÅ"]
    end

    subgraph Mod√®le["Mod√®le du Syst√®me"]
        A["Matrice d'√©tat A"]
        B["Matrice de commande B"]
        C["Matrice d'observation C"]
    end

    subgraph Bruits["Statistiques des Bruits"]
        PHI["Covariance √©tat Œ¶"]
        PSI["Covariance mesure Œ®"]
    end

    subgraph Filtre["Filtre de Kalman"]
        PRED["Pr√©diction"]
        CORR["Correction"]
    end

    subgraph Sorties
        XHAT["Estimation xÃÇ‚Çô‚Çä‚ÇÅ"]
        P["Covariance P‚Çô‚Çä‚ÇÅ"]
        K["Gain K‚Çô‚Çä‚ÇÅ"]
    end

    X0 --> PRED
    U --> PRED
    A --> PRED
    B --> PRED
    PHI --> PRED
    
    PRED --> CORR
    Y --> CORR
    C --> CORR
    PSI --> CORR
    
    CORR --> XHAT
    CORR --> P
    CORR --> K
    
    P -->|"R√©troaction"| PRED

    style Filtre fill:#f5f5f5,stroke:#333,stroke-width:2px
```

### Interpr√©tation du Gain de Kalman

```mermaid
flowchart TB
    subgraph Confiance["Pond√©ration par le Gain K"]
        direction LR
        
        subgraph CasK0["K ‚Üí 0"]
            K0_desc["Covariance mesure Œ® >> Covariance pr√©diction P‚Åª"]
            K0_result["‚Üí On fait confiance au <b>MOD√àLE</b>"]
        end
        
        subgraph CasK1["K ‚Üí I"]
            K1_desc["Covariance pr√©diction P‚Åª >> Covariance mesure Œ®"]
            K1_result["‚Üí On fait confiance aux <b>MESURES</b>"]
        end
    end

    MODEL["Pr√©diction du mod√®le<br/>xÃÇ‚Åª = A xÃÇ + B u"] 
    MESURE["Observation<br/>y = C x + œà"]
    
    MODEL --> |"Pond√©ration (I-K)"| FUSION
    MESURE --> |"Pond√©ration K"| FUSION
    
    FUSION["<b>Fusion optimale</b><br/>xÃÇ = xÃÇ‚Åª + K(y - C xÃÇ‚Åª)"]

    style CasK0 fill:#ffecb3,stroke:#ff8f00
    style CasK1 fill:#c8e6c9,stroke:#388e3c
    style FUSION fill:#e3f2fd,stroke:#1565c0,stroke-width:3px
```

_Par coh√©rence avec les notations du cours : $H \rightarrow C, Q \rightarrow \Phi, R \rightarrow \Psi$_


# Filtre de Kalman - Impl√©mentation
## La recette de cuisine !

1. Initialisation de $\hat{x}$ √† $\hat{x}_0$ : on peut par exemple prendre la valeur $y_0$ renvoy√©e par le capteur.

2. Initialisation de $P$ √† $P_0$ :  on peut prendre la valeur $\Psi$ de la covariance de bruit du capteur.

3. Evolution de $K$ selon :
$$K_{n+1} = (A P_n A^T + \Psi)C^T \times (C A P_n A^T C^T + C \Psi C^T + \Psi)^{-1}$$

4. Evolution de $\hat{x}$ selon :
$$\hat{x}_{n+1} = A \hat{x}_n + B u_n + K_{n+1} [y_{n+1} - C(A \hat{x}_n + B u_n)]$$

5. Evolution de $P$ selon :
$$P_{n+1} = (I - K_{n+1} C)(A P A^T + \Phi) $$


## Est-ce que √ßa marche vraiment en pratique ?
Consid√©rations sur l'√©volution de la variance 

On a √©tablit que $\lim\limits_{n \rightarrow \infty} \mathbb{E}[e_n] = 0$,  ainsi que la formule pour $P_{n+1}$. 

$\rightarrow$ Est-ce qu'on est oblig√© d'attendre l'$\infty$ pour que le filtre commence √† fonctionner ?

$\rightarrow$ En pratique, comment fixer $K_{n+1}$ le gain du filtre ?

**Dispersion** Nous allons travailler sur la dynamique de l'erreur $e_n$ et sa dispersion, pour 

$$P = \mathbb{E}[e_n \times e_n^T]$$

:::{note} Cas 1D
Dans un cas √† une dimension, $P = \mathbb{E}[(\hat{x}_n - x_n)^2] = \mathbb{V}[e_n]$, s'√©crit bien comme la variance de l'erreur.
:::

# Dynamique de l'erreur
## Choix du gain K

**Condition d'optimalit√© du gain du filtre** 

On cherche $K$ de telle sorte que la variance de l'erreur $P = \mathbb{E}[e \times e^T] \sim \mathbb{E}[e]$ soit minimale.

Un condition n√©cessaire est de chercher le gain $K$ tel que $P(k)$ soit extr√™mal, autrement dit que 

$$\forall n \in \mathbb{N}, \frac{\partial P_{n+1}}{\partial K_{n+1}} = 0$$

**Dynamique de l'erreur** En maniuplant les √©quations du filtre, on obtient 

$$
e_{n+1} = (I - K_{n+1} C) A e_n + (K_{n+1} C - I) \phi_n + K_{n+1} \psi_{n+1}
$$
$$
\frac{\partial e_{n+1}}{\partial K_{n+1}} = - C A e_n + C \phi_n + \psi_n 
$$

On cherche le gain $K$ pour √©tablir $\forall n \in \mathbb{N}, \frac{\partial P_{n+1}}{\partial K_{n+1}} = 0$

**Condition suffisante** 
$$\mathbb{E}[\frac{\partial e_{n+1}}{\partial K_{n+1}} e^T_{n+1}] = 0$$

**Formule de $K$ √† partir de $P$**

$$P_{n+1} = (A P_nn A^T + \Psi) C^T \times (C A P_n A^T C^T + C \Phi C^T + \Psi)^{-1}$$

**Formule de $P$ √† partir de $K$** (en prenant en compte $P_0 = \Psi$)

$$P_{n+1} = (I - K_{n+1} C)(A P_n A^T + \Phi)$$

:::{note} Remarque
On ne peut pas obtenir de condition d'optimalit√©, mais simplement d'une relation de r√©currence entre $P$ et $K$. C'est elle qui nous permet d'impl√©menter $K$ en pratique.
:::


## BLUE - Best Linear Unbiased Estimator

**Estimateur sans biais** 

On cherche √† obtenir un estimateur qui satisfait $\lim_{n \rightarrow \infty} \mathbb{E}[e_n] = 0$ (assymptotiquement sans biais).

**Gain optimal** 

On cherche √† r√©gler $K$ pour que les variances d'erreurs soit les plus faibles √† chaque pas de temps, c'est-√†-dire $\frac{\partial P_{n+1}}{\partial K_{n+1}} = 0$.

**R√©currence** 

Dans la mesure o√π nous ne connaissons pas les valeurs vraies $x_n$, nous tirons parti des relations de r√©currence sur l'erreur $e_n$ pour progresser vers une erreur nulle.

C'est cela m√™me qui fait la structure du Filtre Kalman. Et c'est bien pratique dans la mesure o√π le filtre ne d√©pend que des valeurs √† l'√©tat $n$ pour estimer l'√©tat $n+1$ (le filtre est robuste et facile √† mettre en oeuvre).

## Structure stochastique


**Point de d√©part**

- Bruit d'√©tat $\phi \sim \mathcal{N}(0, \Phi)$ (ex : $\Phi$ donn√© par la distibution de temp√©ratures au point de mesure, i.e. la climatologie du lieu),
- Bruit de mesure $\psi \sim \mathcal{N}(0, \Psi)$ (ex : $\Psi$, pr√©cision donn√©e par la fiche technique du capteur de temp√©rature).

**Point d'arriv√©e**
- On mod√©lise $P(x_n| z_n) \sim \mathcal{N}(\hat{x}_n, P_n)$, avec notre estimateur $\hat{x}_n$,
- Avec les propri√©t√©s de l'estimateur, on progresse vers $\mathbb{E}[\hat{x}_n] = x_k$ en gardant une dispersion minimale $\mathbb{E}[(x_n - \hat{x}_n)(x_n - \hat{x_n})^T] = P_n$.


## Impl√©mentation pratique d'un filtre de Kalman

**Hypoth√®ses structurantes** Il reste en pratique √† v√©rifier que les bruits de mesure et d'√©tat sont effectivement des bruits blancs gaussiens, stationnaires.

1. Qu'on a "suffisament capt√©" la dynamique du syst√®me et son observation dans les matrices $A$, $B$ et $C$, pour que les bruits $\Phi$ et $\Psi$ soient effectivement centr√©s.

2. Que les bruits puissent √™tre assimil√©s √† des bruits blancs gaussiens. Cela implique des tests statistiques, et √©ventuellement un travail sur les √©quations pour
centrer et r√©duire le bruit (lien avec le TCL).

3. Qu'il y ait effectivement ind√©pendance entre le bruit de mesure et le bruit d'√©tat. _Exemple : le capteur de temp√©rature qui influence son environnement en 
ralentissant le flux d'air dont il mesure la temp√©rature._

:::{note} En pratique
Il n'y a pas forc√©ment de r√©ponse syst√©matique √† ces questions, simplement un travail de mise au point du filtre, sur un probl√®me donn√© $\rightarrow$ **Coeur du travail de l'ing√©nieur**
:::
 
# Exemple - Estimation d'une tension constante

**Probl√®me** 

Nous cherchons √† estimer une tension constante. Avec, √† disposition, un volt-m√®tre, dont les mesures sont
perturb√©es par un bruit blanc d'√©cart-type $\sigma = 0.1 V$.

![noisy_measurements](../img/constant_voltage_noisy_measurements.png)

**Syst√®me lin√©aire**

$$x_{n+1} = x_{n} + \phi_{n+1}$$
$$z_{n+1} = x_{n+1} + \psi_{n+1}$$


**Equations du Filtre** Dans ce cas simple, $A_f = B_f = C = I = 1$, nous √©tudions alors l'estimation 
$\hat{x}$, la (co)-variance de l'erreur $P_n$ et le gain $K_n$, qui sont les charact√©ristiques essentielles du filtre. 

1. Pr√©dicteur :

$$\hat{x}^-_{n+1} = \hat{x}_{n}$$
$$P^-_{n+1} = P_n + \Phi \; (= P_n + \mathbb{V}[\phi_{n+1}]) $$

2. Correcteur :

$$K_{n+1} = P^-_{n+1} (P^-_{n+1} + \Psi)^{-1} = \frac{P_n + \Phi}{P_{n} + \Phi + \mathbb{V}[\psi_n]}$$
$$\hat{x}_{n+1} = \hat{x}^-_{n+1} + K_{n+1}(z_{n+1} - \hat{x}^-_{n+1}) $$
$$P_{n+1} = (1 - K_{n+1}) P_{n+1}^-$$


**Hypoth√®ses et mod√©lisation**

1. Ici, on suppose une variance sur le bruit d'√©tat : $\Phi = 1 \times 10^{-5}$. C'est √† dire qu'on se laisse la
possibilit√© d'avoir un petit bruit d'√©tat autour de la constante √† mesurer (perturbations √©lectromagn√©tiques ou autre).

2. Choix des conditions initiales : on suppose que la tension √† mesurer est une constante tir√©e d'une loi normale centr√©e.


On fixe alors $x_0 = 0$, et on choisit une valeur arbitraire, mais non-nulle ($P_0 \neq 0$) pour $P_0$. Ici, $P_0 = 1$.



## Vraie valeur, Mesures, et Estimation (50 relev√©s)

![kalman random constant](../img/kalman_1.png)
_Source : Welch & Bishop, Intro to the Kalman Filter"_


## Vitesse de convergence - (Co)-Variance de l'erreur $P_n$

![kalman random constant](../img/kalman_2.png)

_Source : Welch & Bishop, Intro to the Kalman Filter"_


## Influence des bruits $\Psi$ et $\Phi$, avec $\Psi = 1$

![kalman](../img/kalman_3.png)

_Source : Welch & Bishop, Intro to the Kalman Filter"_


## Influence des bruits $\Psi$ et $\Phi$, avec $\Psi = 1 .  10^{-5}$

![kalman](../img/kalman_4.png)

_Source : Welch & Bishop, Intro to the Kalman Filter"_


# Dynamique Non-Lin√©aire - Filtre de Kalman Etendu


Rempla√ßons :
$$x_{n+1} = A x_n + B u_n + \phi_n$$
$$y_{n+1} = C x_{n+1} + \psi_{n+1}$$


Par :
- $x_{n+1} = f(x_n, u_n, \phi_n)$ (mod√®le non-lin√©aire)
- $y_{n+1} = h(x_{n+1}, \psi_{n+1})$ (observateur non-lin√©aire)


:::{note} En m√©t√©orologie
C'est le cas en M√©t√©o o√π le mod√®le repose sur les √©quations de Navier-Stokes (non-lin√©aires), et les observateurs reposent sur les lois non-lin√©aires (ex : r√©flectivit√© radar $R \propto D^6$).
:::

**Solution** : Lin√©ariser les √©quations (mod√®le et observateurs), autour du point de fonctionnement $\hat{x}_n$ estim√©.

**Outil** : les matrices jacobiennes des op√©rateurs $y = h(x)$ et mod√®les $x_{n+1} = f(x_n)$

On note alors :

1. $J^A_{(i,j)} = \frac{\partial f_i}{\partial x_j}$, la jacobienne de $f$ par rapport √† $x$
2. $W_{(i,j)} = \frac{\partial f_i}{\partial \phi_j}$, la jacobienne de $f$ par rapport au bruit d'√©tat $\phi$
3. $J^H_{(i,j)} = \frac{\partial h_i}{\partial x_j}$, la jacobienne de $h$ par rapport √† $x$
4. $V_{(i,j)} = \frac{\partial h_i}{\partial \psi_j}$, la jacobienne de $h$ par rapport au bruit de mesure $\psi$.


1. On utilise directement $f$, et $h$ directement pour le calcul de $\hat{x}^-_n$ √† priori et l'√©valuation de la mesure $h(y_n)$.
2. On utilise les matrices jacobiennes dans le calcul des covariances :

$P^-_{n+1} = A P_{n} A^T + \Phi$ devient $P^-_{n+1} = J^A P_n (J^{A})^T + W \Phi W^T$ 

$K_{n+1}$ devient $K_{n+1} = P^-_{n+1} (J^H)^T (J^H P^-_{n+1} (J^H)^T + V \Psi V^T)$

$P_{n+1}$ devient $P_{n+1} = (I - K_{n+1} J^H) P^-_{n+1}$

o√π $J^A$ et $J^H$ ont √©t√© √©valu√©es au point $\hat{x}_n$


**En M√©t√©o** 

On ne s'√©tonnera pas de trouver les termes de **Tangent-Lin√©aire** (TL) et **Adjoint** (AD) en assimilation de donn√©es, il s'agit respectivement de la **jacobienne** $J^A$ et de sa **transpos√©e** $(J^A)^T$
_(plus exactement du conjugu√© de sa transpos√©e si on travaille sur un espace complexe)_.

**En pratique** 

L√† encore, le **coeur du travail d'ing√©nieur** est d'obtenir les op√©rateurs ad√©quats (en respectant de mani√®re empirique les hypoth√®ses sur les distributions).


### Sch√©ma du Filtre de Kalman √âtendu (EKF)

```mermaid
flowchart TB
    subgraph EKF["Filtre de Kalman √âtendu"]
        direction TB
        
        subgraph LINEAR["Filtre de Kalman Standard"]
            L1["Matrices constantes A, C"]
            L2["Dynamique lin√©aire"]
        end
        
        subgraph NONLINEAR["Extension Non-Lin√©aire"]
            N1["Fonctions f(x), h(x)"]
            N2["Lin√©arisation locale"]
            N3["Jacobiennes J<sup>A</sup>, J<sup>H</sup>"]
        end
        
        LINEAR -->|"G√©n√©ralisation"| NONLINEAR
    end
    
    subgraph STEPS["√âtapes EKF"]
        S1["1. Pr√©diction avec f(xÃÇ‚Çô)"]
        S2["2. Calcul de J<sup>A</sup> en xÃÇ‚Çô"]
        S3["3. Propagation covariance<br/>P‚Åª = J<sup>A</sup> P (J<sup>A</sup>)·µÄ + WŒ¶W·µÄ"]
        S4["4. Calcul de J<sup>H</sup> en xÃÇ‚Åª"]
        S5["5. Gain de Kalman avec J<sup>H</sup>"]
        S6["6. Correction avec h(xÃÇ‚Åª)"]
        
        S1 --> S2 --> S3 --> S4 --> S5 --> S6
    end

    style EKF fill:#f5f5f5,stroke:#333
    style LINEAR fill:#e3f2fd,stroke:#1565c0
    style NONLINEAR fill:#fff8e1,stroke:#ff8f00
    style STEPS fill:#e8f5e9,stroke:#2e7d32
```


# Synth√®se 
## Qu'est-ce qu'un filtre de Kalman d√©j√† ?

- **Fondamentaux**
    - La recette et le fonctionnement de **pr√©dicteur-correcteur** pour le filtre Kalman,
    - (Th√©orie), c'est un estimateur **assymptotiquement sans biais**, et avec une **variance d'erreur √† minimiser**.

- **Construction**
    - La d√©marche r√©cursive de construction du filtre,  _utile pour comprendre le filtre "pas-√†-pas"_.

- **En pratique, avec l'algorithme √† disposition**
    - Comment r√©gler le gain du filtre ?
    - Comment respecter au mieux les hypoth√®ses de bruit blanc gaussien ?
    - Comment lin√©ariser le mod√®le et les observateurs de mesure ?

:::{note} Aspect pratiques
En gardant √† l'esprit que le Filtre de Kalman est plut√¥t un filtre simple et robuste, et que le travail consiste √† construire les "bonnes" matrices de covariance, et r√©gler le gain du filtre.
:::


# R√©f√©rences 

## R√©f√©rences principales

### Articles fondateurs

1. **Kalman, R. E.** (1960). *A New Approach to Linear Filtering and Prediction Problems*. Journal of Basic Engineering, 82(1), 35-45. 
   - L'article original de Kalman, fondement th√©orique du filtre.
   - [DOI: 10.1115/1.3662552](https://doi.org/10.1115/1.3662552)

2. **Kalman, R. E., & Bucy, R. S.** (1961). *New Results in Linear Filtering and Prediction Theory*. Journal of Basic Engineering, 83(1), 95-108.
   - Extension au cas continu (filtre de Kalman-Bucy).

### Tutoriels et introductions

3. **Welch, G., & Bishop, G.** (2006). *An Introduction to the Kalman Filter*. University of North Carolina at Chapel Hill, TR 95-041.
   - Excellente introduction p√©dagogique avec exemples.
   - [PDF disponible](https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf)

4. **Herscovici-Schiller, O.** *Introduction au filtrage de Kalman et √† la commande optimale*. ONERA.
   - Cours en fran√ßais, approche rigoureuse.
   - [PDF disponible](https://www.onera.fr/sites/default/files/270/poly_Kalman_Herscovici.pdf)

5. **Chardon, G.** *Filtrage de Kalman*. CentraleSup√©lec.
   - Notes de cours en fran√ßais.
   - [PDF disponible](https://gilleschardon.fr/fc/kalman/kalman.pdf)


## Ouvrages de r√©f√©rence

### Th√©orie de l'estimation et du filtrage

6. **Simon, D.** (2006). *Optimal State Estimation: Kalman, H‚àû, and Nonlinear Approaches*. Wiley-Interscience.
   - Ouvrage complet couvrant le filtre de Kalman et ses extensions (EKF, UKF, filtres particulaires).
   - ISBN: 978-0471708582

7. **Anderson, B. D. O., & Moore, J. B.** (1979). *Optimal Filtering*. Prentice-Hall. (R√©√©dit√© par Dover, 2005)
   - Classique incontournable sur la th√©orie du filtrage optimal.
   - ISBN: 978-0486439389

8. **Bar-Shalom, Y., Li, X. R., & Kirubarajan, T.** (2001). *Estimation with Applications to Tracking and Navigation*. Wiley.
   - R√©f√©rence pour les applications en pistage et navigation.
   - ISBN: 978-0471416555

9. **Jazwinski, A. H.** (1970). *Stochastic Processes and Filtering Theory*. Academic Press. (R√©√©dit√© par Dover, 2007)
   - Traitement math√©matique rigoureux des processus stochastiques et du filtrage.
   - ISBN: 978-0486462745


### Processus stochastiques et syst√®mes dynamiques

10. **√òksendal, B.** (2003). *Stochastic Differential Equations: An Introduction with Applications* (6th ed.). Springer.
    - Fondements math√©matiques des √©quations diff√©rentielles stochastiques.
    - ISBN: 978-3540047582

11. **Papoulis, A., & Pillai, S. U.** (2002). *Probability, Random Variables and Stochastic Processes* (4th ed.). McGraw-Hill.
    - Manuel classique sur les processus al√©atoires.
    - ISBN: 978-0071226615


## Extensions et variantes du filtre de Kalman

### Filtre de Kalman √âtendu (EKF)

12. **Julier, S. J., & Uhlmann, J. K.** (1997). *A New Extension of the Kalman Filter to Nonlinear Systems*. Proceedings of AeroSense.
    - Introduction du filtre de Kalman Unscented (UKF), alternative √† l'EKF.

13. **Wan, E. A., & Van Der Merwe, R.** (2000). *The Unscented Kalman Filter for Nonlinear Estimation*. Proceedings of the IEEE Adaptive Systems for Signal Processing, Communications, and Control Symposium.
    - Description d√©taill√©e de l'UKF et comparaison avec l'EKF.


### Filtres particulaires

14. **Arulampalam, M. S., Maskell, S., Gordon, N., & Clapp, T.** (2002). *A Tutorial on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking*. IEEE Transactions on Signal Processing, 50(2), 174-188.
    - Tutoriel de r√©f√©rence sur les filtres particulaires.
    - [DOI: 10.1109/78.978374](https://doi.org/10.1109/78.978374)

15. **Doucet, A., De Freitas, N., & Gordon, N.** (Eds.). (2001). *Sequential Monte Carlo Methods in Practice*. Springer.
    - Ouvrage collectif de r√©f√©rence sur les m√©thodes de Monte-Carlo s√©quentielles.
    - ISBN: 978-0387951461


## Applications sp√©cifiques

### Assimilation de donn√©es en m√©t√©orologie

16. **Evensen, G.** (2009). *Data Assimilation: The Ensemble Kalman Filter* (2nd ed.). Springer.
    - R√©f√©rence pour le filtre de Kalman d'ensemble (EnKF) en g√©osciences.
    - ISBN: 978-3642037108

17. **Kalnay, E.** (2003). *Atmospheric Modeling, Data Assimilation and Predictability*. Cambridge University Press.
    - Ouvrage de r√©f√©rence sur l'assimilation de donn√©es atmosph√©riques.
    - ISBN: 978-0521796293

18. **Asch, M., Bocquet, M., & Nodet, M.** (2016). *Data Assimilation: Methods, Algorithms, and Applications*. SIAM.
    - Traitement moderne et complet de l'assimilation de donn√©es.
    - ISBN: 978-1611974539


### Navigation et robotique

19. **Thrun, S., Burgard, W., & Fox, D.** (2005). *Probabilistic Robotics*. MIT Press.
    - R√©f√©rence pour l'utilisation du filtre de Kalman en robotique mobile.
    - ISBN: 978-0262201629

20. **Groves, P. D.** (2013). *Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems* (2nd ed.). Artech House.
    - Applications du filtrage de Kalman √† la navigation int√©gr√©e.
    - ISBN: 978-1608070053


### Traitement du signal et communications

21. **Haykin, S.** (Ed.). (2001). *Kalman Filtering and Neural Networks*. Wiley.
    - Liens entre filtrage de Kalman et r√©seaux de neurones.
    - ISBN: 978-0471369981

22. **Brown, R. G., & Hwang, P. Y. C.** (2012). *Introduction to Random Signals and Applied Kalman Filtering* (4th ed.). Wiley.
    - Introduction accessible avec nombreuses applications.
    - ISBN: 978-0470609699


## Ressources en ligne

### Cours et tutoriels

23. **Labbe, R.** *Kalman and Bayesian Filters in Python*. 
    - Livre interactif (Jupyter notebooks) tr√®s p√©dagogique.
    - [GitHub Repository](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)

24. **Faragher, R.** (2012). *Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation*. IEEE Signal Processing Magazine, 29(5), 128-132.
    - D√©rivation intuitive du filtre de Kalman.
    - [DOI: 10.1109/MSP.2012.2203621](https://doi.org/10.1109/MSP.2012.2203621)


### Impl√©mentations

25. **FilterPy** - Biblioth√®que Python pour les filtres de Kalman et filtres bay√©siens.
    - [Documentation](https://filterpy.readthedocs.io/)

26. **Robot Operating System (ROS)** - Packages de filtrage pour la robotique.
    - `robot_localization`: [Wiki ROS](http://wiki.ros.org/robot_localization)


## Pour aller plus loin

### Contr√¥le optimal et LQG

27. **√Östr√∂m, K. J.** (1970). *Introduction to Stochastic Control Theory*. Academic Press. (R√©√©dit√© par Dover, 2006)
    - Lien entre filtrage de Kalman et contr√¥le optimal (LQG).
    - ISBN: 978-0486445311

28. **Lewis, F. L., Vrabie, D., & Syrmos, V. L.** (2012). *Optimal Control* (3rd ed.). Wiley.
    - Traitement complet du contr√¥le optimal incluant le probl√®me LQG.
    - ISBN: 978-0470633496


### Approches bay√©siennes modernes

29. **S√§rkk√§, S.** (2013). *Bayesian Filtering and Smoothing*. Cambridge University Press.
    - Approche unifi√©e du filtrage bay√©sien.
    - ISBN: 978-1107619289
    - [PDF disponible](https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf)

30. **Murphy, K. P.** (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
    - Chapitre 18 sur les mod√®les d'√©tat et le filtrage de Kalman dans un contexte machine learning.
    - ISBN: 978-0262018029